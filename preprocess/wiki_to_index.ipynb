{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-13T01:54:24.953097Z",
     "iopub.status.busy": "2024-12-13T01:54:24.952848Z",
     "iopub.status.idle": "2024-12-13T01:54:42.586529Z",
     "shell.execute_reply": "2024-12-13T01:54:42.585506Z",
     "shell.execute_reply.started": "2024-12-13T01:54:24.953071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting faiss-gpu\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Collecting langchain-core<0.4.0,>=0.3.24 (from langchain)\n",
      "  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.3,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.2.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.7)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.46.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
      "Collecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.4)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.0)\n",
      "Downloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.2.3-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu, packaging, httpx-sse, requests-toolbelt, pydantic-settings, langsmith, langchain-core, sentence-transformers, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 21.3\n",
      "    Uninstalling packaging-21.3:\n",
      "      Successfully uninstalled packaging-21.3\n",
      "  Attempting uninstall: requests-toolbelt\n",
      "    Found existing installation: requests-toolbelt 0.10.1\n",
      "    Uninstalling requests-toolbelt-0.10.1:\n",
      "      Successfully uninstalled requests-toolbelt-0.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 24.10.1 requires cubinlinker, which is not installed.\n",
      "cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cudf 24.10.1 requires libcudf==24.10.*, which is not installed.\n",
      "cudf 24.10.1 requires ptxcompiler, which is not installed.\n",
      "cuml 24.10.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "cuml 24.10.0 requires cuvs==24.10.*, which is not installed.\n",
      "cuml 24.10.0 requires nvidia-cublas, which is not installed.\n",
      "cuml 24.10.0 requires nvidia-cufft, which is not installed.\n",
      "cuml 24.10.0 requires nvidia-curand, which is not installed.\n",
      "cuml 24.10.0 requires nvidia-cusolver, which is not installed.\n",
      "cuml 24.10.0 requires nvidia-cusparse, which is not installed.\n",
      "dask-cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\n",
      "pylibcudf 24.10.1 requires libcudf==24.10.*, which is not installed.\n",
      "cudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\n",
      "cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
      "dask-cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\n",
      "distributed 2024.9.0 requires dask==2024.9.0, but you have dask 2024.11.2 which is incompatible.\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\n",
      "jupyterlab 4.3.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n",
      "kfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n",
      "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "plotnine 0.14.3 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\n",
      "pylibcudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\n",
      "rapids-dask-dependency 24.10.0a0 requires dask==2024.9.0, but you have dask 2024.11.2 which is incompatible.\n",
      "rapids-dask-dependency 24.10.0a0 requires dask-expr==1.1.14, but you have dask-expr 1.1.19 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "ydata-profiling 4.12.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed faiss-gpu-1.7.2 httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 langchain-core-0.3.24 langchain-text-splitters-0.3.2 langsmith-0.2.3 packaging-24.2 pydantic-settings-2.6.1 requests-toolbelt-1.0.0 sentence-transformers-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community faiss-gpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T01:54:42.589166Z",
     "iopub.status.busy": "2024-12-13T01:54:42.588889Z",
     "iopub.status.idle": "2024-12-13T01:54:42.645521Z",
     "shell.execute_reply": "2024-12-13T01:54:42.644703Z",
     "shell.execute_reply.started": "2024-12-13T01:54:42.589139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ruwiki-tables-and-lists/wiki_tables_and_lists.jsonl\n",
      "/kaggle/input/ruwiki-valid-tables/wiki_dump.jsonl\n",
      "/kaggle/input/bert2bert-4b-300e/results/runs/Dec11_01-51-37_075c355daadc/events.out.tfevents.1733881900.075c355daadc.23.0\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/config.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/trainer_state.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/training_args.bin\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/tokenizer.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/tokenizer_config.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/scheduler.pt\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/model.safetensors\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/special_tokens_map.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/optimizer.pt\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/vocab.txt\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/rng_state.pth\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/generation_config.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/config.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/trainer_state.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/training_args.bin\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/tokenizer.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/tokenizer_config.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/scheduler.pt\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/model.safetensors\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/special_tokens_map.json\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/optimizer.pt\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/vocab.txt\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/rng_state.pth\n",
      "/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/generation_config.json\n",
      "/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/run-qeoxzw3k.wandb\n",
      "/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/logs/debug.log\n",
      "/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/logs/debug-internal.log\n",
      "/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/files/output.log\n",
      "/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/files/requirements.txt\n",
      "/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/files/wandb-metadata.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T02:01:54.683729Z",
     "iopub.status.busy": "2024-12-13T02:01:54.683356Z",
     "iopub.status.idle": "2024-12-13T02:19:22.704522Z",
     "shell.execute_reply": "2024-12-13T02:19:22.703076Z",
     "shell.execute_reply.started": "2024-12-13T02:01:54.683698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs for inference...\n",
      "Loading existing FAISS index from /kaggle/working/faiss_tables_index_checkpoint\n",
      "Resuming from table #30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 10905it [00:29, 414.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 40000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 20927it [00:58, 415.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 50000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 30804it [01:27, 403.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 60000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 40678it [01:57, 352.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 70000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 50914it [02:28, 341.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 80000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 60581it [02:59, 341.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 90000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 70665it [03:30, 332.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 100000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 80749it [04:01, 346.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 110000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 90907it [04:34, 324.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 120000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 100858it [05:05, 313.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 130000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 110354it [05:38, 198.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 140000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 120339it [06:09, 227.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 150000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 130598it [06:42, 292.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 160000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 140794it [07:14, 283.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 170000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 150632it [07:48, 254.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 180000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 160689it [08:20, 265.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 190000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 170850it [08:52, 272.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 200000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 180611it [09:24, 261.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 210000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 190463it [09:58, 230.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 220000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 200147it [10:32, 182.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 230000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 210638it [11:06, 251.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 240000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 220345it [11:41, 149.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 250000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 230541it [12:15, 213.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 260000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 240614it [12:48, 241.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 270000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 250860it [13:25, 224.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 280000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 260706it [14:02, 235.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 290000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 270638it [14:40, 207.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 300000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 280682it [15:16, 212.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 310000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 290514it [15:53, 186.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 320000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 300308it [16:30, 118.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 330000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 310835it [17:06, 210.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Processed 340000 tables so far.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tables: 312658it [17:12, 302.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\n",
      "Final processed tables: 342658\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pydantic import PrivateAttr\n",
    "import re\n",
    "\n",
    "# Параметры\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "INPUT_FILE = \"/kaggle/input/ruwiki-valid-tables/wiki_dump.jsonl\"\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "CHECKPOINT_NAME = \"faiss_tables_index_checkpoint\"\n",
    "BATCH_SIZE = 1000\n",
    "CHECKPOINT_INTERVAL = 10\n",
    "RESUME_FROM_CHECKPOINT = True\n",
    "\n",
    "class MultiGPUHuggingFaceEmbeddings(HuggingFaceEmbeddings):\n",
    "    _tokenizer: AutoTokenizer = PrivateAttr()\n",
    "    _model: nn.Module = PrivateAttr()\n",
    "    _device: str = PrivateAttr()\n",
    "    _half: bool = PrivateAttr()\n",
    "    _normalize_embeddings: bool = PrivateAttr(default=False)\n",
    "\n",
    "    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME, device: str = None, half: bool = True, **kwargs):\n",
    "        super().__init__(model_name=model_name, **kwargs)\n",
    "        \n",
    "        encode_kwargs = kwargs.get(\"encode_kwargs\", {})\n",
    "        self._normalize_embeddings = encode_kwargs.get(\"normalize_embeddings\", True)  # включаем нормализацию\n",
    "\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self._model = AutoModel.from_pretrained(model_name)\n",
    "        self._model.eval()\n",
    "\n",
    "        # Проверка на количество GPU\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs for inference...\")\n",
    "            self._model = nn.DataParallel(self._model)\n",
    "            self._device = 'cuda'\n",
    "        else:\n",
    "            self._device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self._model.to(self._device)\n",
    "\n",
    "        if half and 'cuda' in self._device:\n",
    "            self._model.half()\n",
    "\n",
    "        self._half = half\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        # Используем mean pooling для получения sentence embeddings\n",
    "        batch_size = 256\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            inputs = self._tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "            inputs = {k: v.to(self._device) for k,v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self._model(**inputs)\n",
    "                last_hidden_state = outputs.last_hidden_state\n",
    "                attention_mask = inputs['attention_mask']\n",
    "\n",
    "                # Mean Pooling\n",
    "                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "                sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "                cls_emb = (sum_embeddings / sum_mask).float().cpu().numpy()\n",
    "\n",
    "                if self._normalize_embeddings:\n",
    "                    norm = (cls_emb**2).sum(axis=1, keepdims=True)**0.5\n",
    "                    cls_emb = cls_emb / norm\n",
    "                embeddings.extend(cls_emb.tolist())\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.embed_documents([text])[0]\n",
    "\n",
    "def create_empty_faiss_index(embedding_model, distance_strategy=DistanceStrategy.COSINE):\n",
    "    import faiss\n",
    "    from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "    # Получаем размерность через фиктивный текст\n",
    "    dummy_emb = embedding_model.embed_documents([\"hello\"])\n",
    "    dim = len(dummy_emb[0])\n",
    "\n",
    "    if distance_strategy == DistanceStrategy.COSINE:\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    docstore = InMemoryDocstore({})\n",
    "    vectorstore = FAISS(\n",
    "        index=index,\n",
    "        docstore=docstore,\n",
    "        index_to_docstore_id={},\n",
    "        embedding_function=embedding_model,\n",
    "        distance_strategy=distance_strategy\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "def create_or_load_faiss_index(embedding_model):\n",
    "    checkpoint_path = os.path.join(OUTPUT_DIR, CHECKPOINT_NAME)\n",
    "    if RESUME_FROM_CHECKPOINT and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading existing FAISS index from {checkpoint_path}\")\n",
    "        vectorstore = FAISS.load_local(checkpoint_path, embedding_model, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        vectorstore = create_empty_faiss_index(embedding_model, distance_strategy=DistanceStrategy.COSINE)\n",
    "    return vectorstore\n",
    "\n",
    "def save_faiss_index(vectorstore):\n",
    "    checkpoint_path = os.path.join(OUTPUT_DIR, CHECKPOINT_NAME)\n",
    "    vectorstore.save_local(checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "def main():\n",
    "    embedding_model = MultiGPUHuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        half=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "        multi_process=False\n",
    "    )\n",
    "\n",
    "    vectorstore = create_or_load_faiss_index(embedding_model)\n",
    "\n",
    "    processed_tables_count = 0\n",
    "    offset_file = os.path.join(OUTPUT_DIR, \"offset.txt\")\n",
    "    if RESUME_FROM_CHECKPOINT and os.path.exists(offset_file):\n",
    "        with open(offset_file, \"r\") as f:\n",
    "            processed_tables_count = int(f.read().strip())\n",
    "        print(f\"Resuming from table #{processed_tables_count}\")\n",
    "\n",
    "    docs_batch = []\n",
    "    batch_counter = 0\n",
    "\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for _ in range(processed_tables_count):\n",
    "            f.readline()\n",
    "\n",
    "        for line in tqdm(f, desc=\"Processing tables\"):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            entry = json.loads(line)\n",
    "            uuid = entry.get(\"uuid\", \"\")\n",
    "            context_before = entry.get(\"context_before\", \"\") or \"\"\n",
    "            caption = entry.get(\"caption\", \"\") or \"\"\n",
    "            header = entry.get(\"header\", [])\n",
    "            data = entry.get(\"data\", [])\n",
    "            uuid_text = re.sub(r'[^А-Яа-яЁё_]+', '', uuid)\n",
    "            uuid_text = uuid_text.replace('_', ' ')\n",
    "            context_text = '\\n'.join('\\n'.join(inner_text) for inner_text in context_before)\n",
    "            header_names = [col[\"name\"] for col in header]\n",
    "            header_line = \" | \".join(header_names)\n",
    "\n",
    "            table_lines = []\n",
    "            for row in data:\n",
    "                row_values = [cell[1] for cell in row] if row and isinstance(row[0], list) else row\n",
    "                table_lines.append(\" | \".join(row_values))\n",
    "            table_text = \"\\n\".join(table_lines)\n",
    "\n",
    "            full_text = f\"{uuid_text}\\n{context_text}\\n{caption}\\n{header_line}\\n{table_text}\".strip()\n",
    "            \n",
    "            doc = LangchainDocument(\n",
    "                page_content=full_text,\n",
    "                metadata={\n",
    "                    \"uuid\": uuid,\n",
    "                    \"header\": header_names\n",
    "                }\n",
    "            )\n",
    "            docs_batch.append(doc)\n",
    "\n",
    "            processed_tables_count += 1\n",
    "\n",
    "            if len(docs_batch) >= BATCH_SIZE:\n",
    "                vectorstore.add_documents(docs_batch)\n",
    "                docs_batch = []\n",
    "                batch_counter += 1\n",
    "\n",
    "                if batch_counter % CHECKPOINT_INTERVAL == 0:\n",
    "                    save_faiss_index(vectorstore)\n",
    "                    with open(offset_file, \"w\") as f_off:\n",
    "                        f_off.write(str(processed_tables_count))\n",
    "                    print(f\"Processed {processed_tables_count} tables so far.\")\n",
    "            # if processed_tables_count>=30000:\n",
    "            #     break\n",
    "\n",
    "    if docs_batch:\n",
    "        vectorstore.add_documents(docs_batch)\n",
    "\n",
    "    save_faiss_index(vectorstore)\n",
    "    with open(offset_file, \"w\") as f_off:\n",
    "        f_off.write(str(processed_tables_count))\n",
    "    print(f\"Final processed tables: {processed_tables_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6281445,
     "sourceId": 10170794,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6284201,
     "sourceId": 10174404,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6290599,
     "sourceId": 10183147,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
