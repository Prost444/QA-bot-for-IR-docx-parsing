# QA-bot-for-IR-docx-parsing

This repository contains the solution for the NLP case from Rosneft &amp; MIREA AI hackathon. It provides a method for parsing docx files or wiki_dump pages, extracting tables and lists for further processing and embedding them into a QA model.

Этот репозиторий предоставляет решение для обработки .docx документов, извлечения структурированных данных (например, таблиц и списков) и интеграции их в систему для ответа на вопросы с использованием современных NLP-моделей. Проект разработан для масштабируемой обработки документов и интеграции с трансформерными языковыми моделями.

---

### Постановка задачи:

Требовалось предложить метод парсинга таблиц и списков документов (docx) для векторной базы данных для IR-системы в составе QA чат-бота.
Для построения IR-системы с векторной базой данных было выбрано 2 подхода:

- **Baseline**: собрать и обучить на табличных данных модель seq-to-seq. Модель должна была обучиться принимать текстовые запросы от пользователей и с помощью encoder-decoder архитектуры выдавать ответы с информацией из обучающей выборки.

- **Pro**: При помощи выбранного векторизатора создать FAISS базу данных с эмбеддингами табличных документов, по которой будет выполняться быстрый поиск контекста для запроса пользователя. Далее запрос и полученный из базы данных контекст подавался LLM для извлечения нужной информации, суммаризации и представления ответа.

---

### Данные для обучения:

Организаторами хакатона было предложено извлекать информацию о таблицах и списках для векторизованной базы данных из .docx файлов. Однако для создания информативной базы данных, а тем более обучения (дообучения) encoder-decoder модели требовалась выборка из большого количества docx файлов с миллионами информативных таблиц и списков. Найти подобную выборку нам не удалось, поэтому было принято решение создать ее самостоятельно на основе всех таблиц и списков, представленных в русской Википедии.

# Шаги создания:

1. Установка последнего дампа русской Википедии

   В целях ускорения преобразования вики-файлов в docx было принято решение отказаться от парсинга интернет-страниц в пользу скачивания бэкапа и обработки всех страниц исключительно локально.

2. Экстракция в docx файлы всех таблиц и списков с каждой страницы дампа Википедии.

   Код приведен в файле `wiki_parse_to_docx.ipynb`.

---

# Датасет для обучения

Для информативной репрезентации таблиц, понятной для NLP-систем, все таблицы и списки было решено хранить в JSON формате (при этом все списки преобразуются к одномерным таблицам). Для сборки репрезентативного датасета использованы 2 парсера (каталог `preprocess`):

1. `docx_to_json.py` - собирает JSONL файл из предоставленного каталога с docx файлами.

2. `extract_wiki_tables.sh` - парсер от создателей TaBERT, оптимизированный под новые версии Python и использование русского языка и захват списков. Парсит в JSONL файл напрямую из дампа Википедии.

Оба парсера также захватывают контекст, находящийся перед таблицей.
Такое извлечение информации показало отличные результаты на CPU с пиковой скоростью обработки до 300000 страниц в час. С их помощью были собраны все таблицы и [списки русской Википедии](https://drive.google.com/file/d/1vn8SfdPBgS6pA4fZLpyqhkemHtewENL7/view?usp=drive_link) и файл с исключительно [информативными таблицами](https://drive.google.com/file/d/1w5K3LYRoLUIJ8TtdrkZbPjREf5rqj-NY/view?usp=drive_link) (минимум 4 строки и 3 столбца).

---

### Обучение представлений:

Для бейзлайнового решения было принято дообучать `DeepPavlov/rubert-base-cased` для задачи seq-to-seq генерации по запросу. Код с построением нужной архитектуры и обучением приведен в файле `llm-with-ir.ipynb`. Для дообучения использовались первые 5000 строк из `wiki_dump.jsonl`. Модель получала текст из таблицы и генерировала контекст. Лосс - кросс-энтропия. Графики представлены ниже

![telegram-cloud-photo-size-2-5388572639825621342-y](https://github.com/user-attachments/assets/81d420ce-8b01-4453-9d9a-e39a6bb03a15)
![telegram-cloud-photo-size-2-5388572639825621341-y](https://github.com/user-attachments/assets/cbf9cbbe-0d98-4815-8c71-ea59d316baef)

Из-за нехватки времени было невозможно обучить модель на большом кластере таблиц, из-за чего она начала переобучаться уже после 8-й эпохи. В дальнейшем мы использовали чекпоинт этой модели с минимальным eval-лоссом (до начала переобучения).

Метрики по этой модели:

**Bert score:**

| precision | recall | f1 |
|-----------|--------|----|
| 0.4167    | 0.5872 | 0.4853 |

| Rouge-L-P | Rouge-L-R | Rouge-L-F |
|-----------|-----------|-----------|
| 0.00315   | 0.79193   | 0.79193   |

---

### Создание FAISS-INDEX:

Ключевым этапом создания векторизованной базы данных выступил выбор модели векторизации. Было протестировано множество вариантов готовых моделей с Hugging Face, использование encoder части дообученной seq-to-seq модели. Векторизировались как датасеты с информативными таблицами, так и полные датасеты. В ходе векторизации заголовок страницы (имя файла), контекст до таблицы, заголовки столбцов и форматированный текст таблицы (также пробовали каждую строку отдельно) сливались в один текст и подавались энкодером модели.

Индекс строился с использованием `IndexFlatIP`, нормализации и близости по косинусному расстоянию. На всех моделях результат поиска был неудовлетворительным (почему - покажем в разделе про метрики). Кроме модели `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`, которая строила самые информативные эмбеддинги. Проверить качество поиска можно загрузив наш чекпоинт индекса и запустив блокнот `handsome_faiss.ipynb` (для отдельного запроса). Также вы можете создать собственный индекс на основе ваших docx файлов с нашим векторизатором при помощи модуля `nlp`.

Код создания векторизованной базы данных (FAISS-INDEX) на основе таблиц Википедии приведен в файле `wiki_to_index` (рекомендуем запускать на кластере из 2-х GPU T4). Код создания той же базы данных напрямую из docx файлов приведен в файле `nlp.py`. Готовые файлы для восстановления индекса, основанного на [информативных таблицах Википедии](https://drive.google.com/drive/folders/1-Tf7lbhTVmndmB5xKGTTE5r3eWNF8mih?usp=drive_link).

---

### Дополнительная суммаризация при помощи LLM:

Для суммаризации ответов мы используем [T-lite-it-1.0](https://huggingface.co/t-tech/T-lite-it-1.0) - самую новую русскоязычную языковую модель от Т-банка с 8 миллиардами параметров. Модель разработана специально для задач, связанных с обработкой информации в корпоративной среде, что подходит для суммаризации отчётов компаний. Ее интеграция для последующих вызовов приведена в модуле `llm`.

Мы выбрали 4-битную квантизацию для повышения скорости работы и уменьшения расхода памяти на архитектуре CUDA с видеокартой NVIDIA GTX 4070 Ti.

**Bert-score:**

| precision | recall  | f1     |
|-----------|---------|--------|
| 0.6253    | 0.5997  | 0.6115 |

| Rouge-L-P | Rouge-L-R | Rouge-L-F |
|-----------|-----------|-----------|
| 0.00289   | 0.96692   | 0.96692   |

---

### Метрики:

Выбор метрик в подобных задачах – это всегда баланс между практичностью, интерпретируемостью и согласованностью с реальными критериями качества ответа. Несмотря на то, что на рынке существует широкий спектр метрик, мы сосредоточились на BertScore и вариантах Rouge-L не случайно:

**Сочетание лексического и семантического анализа:**

Rouge-L измеряет степень лексического пересечения генерации с эталонным ответом. Оно отслеживает Longest Common Subsequence (LCS) между ответом модели и референсом, что позволяет оценить непрерывные фрагменты совпадений, а не просто отдельные токены. Это особенно важно, когда необходимо убедиться, что ответ не просто содержит отдельные релевантные слова, а структурно приближен к эталону (и не повторяет запрос точь-в-точь).

BertScore, в свою очередь, фокусируется не только на лексическом совпадении, но и на семантической близости между ответом модели и референсами. Данная метрика использует эмбеддинги, полученные из языковой модели, чтобы сопоставить токены на уровне смысловых векторов, а не исключительно их лексическую форму. Таким образом, BertScore может «прощать» перефразирование, синонимы и альтернативные грамматические конструкции, если смысл сохраняется.

В отношении RAG систем топ по метрикам выстроился довольно прозрачно следующим образом:

|                            | bert-pr | bert-re | bert-f1 | Rouge-L-P | Rouge-L-R | Rouge-L-F |
|----------------------------|---------|---------|---------|-----------|-----------|-----------|
| paraphrase-multilingual    | 0.6252  | 0.5997  | 0.6115  | 0.00285   | 0.966923  | 0.9669236 |
| DeepPavlov_for_Panacea     | 0.5903  | 0.5655  | 0.5778  | 0.00288   | 0.9669200 | 0.9669200 |
| sentence-rubert            | 0.5899  | 0.5652  | 0.5775  | 0.00288   | 0.9669198 | 0.9669198 |
| sentence-rubert на строках | 0.5896  | 0.5649  | 0.5772  | 0.00289   | 0.9669196 | 0.9669196 |
| наш-дообученный-энкодер    | 0.5894  | 0.5647  | 0.5770  | 0.00302   | 0.9669194 | 0.9669194 |

Также видим колоссальный прирост в осознанности и качестве нового решения:

| Metric         | Seq-to-Seq | RAG System | Change (%) |
|----------------|------------|------------|------------|
| Bert-Precision | 0.4167     | 0.6253     | +50.0%     |
| Bert-Recall    | 0.5872     | 0.5997     | +2.1%      |
| Bert-F1        | 0.4853     | 0.6115     | +26.0%     |
| Rouge-L-P      | 0.003153   | 0.002885   | -9.3%      |
| Rouge-L-R      | 0.7919     | 0.9669     | +18.1%     |
| Rouge-L-F      | 0.7919     | 0.9669     | +18.1%     |

Такие низкие значения Rouge-L-P обусловлены тем, что модель использует минимум токенов из запроса. То есть создает и выдает длинный ответ с новыми словами.

---

### Summary:

- **Парсинг документов:**

    Использует библиотеку `docx` для извлечения структурированных данных, таких как таблицы и списки.

    Обрабатывает и форматирует извлеченные данные в единый контекст для использования в NLP-задачах.

    Чтобы загрузить русскую NLP модель, выполни команду
    ```batch
    python -m spacy download ru_core_news_sm
    ```

- **Работа с эмбеддингами:**

    Реализована поддержка многопроцессорных вычислений с использованием библиотеки `sentence-transformers`.

    Обеспечивает создание, сохранение и загрузку FAISS-векторных индексов для эффективного поиска по схожести.

- **Ответы на вопросы:**

    Интеграция с трансформерными языковыми моделями с использованием библиотек `transformers` и `torch`.

    Настраиваемая генерация ответов на основе предоставленного контекста, поддерживая лаконичные и релевантные ответы.

- **Ключевые компоненты:**

    `nlp.py`: Обрабатывает .docx файлы, извлекает их содержимое (таблицы, списки, контекст) и преобразует их в совместимый формат для использования с Langchain.
    	
    `llm.py`: Класс для взаимодействия с трансформерными языковыми моделями, обеспечивающий генерацию ответов на основе контекста и вопросов.
    	
    `multi_gpu_embeddings.py`: Добавляет поддержку многопроцессорных вычислений для генерации эмбеддингов предложений и управления FAISS-векторными индексами.

- **Технологический стек:**

    Используемые библиотеки: `torch`, `transformers`, `sentence-transformers`, `docx`, `faiss`, `langchain`.

    Проект оптимизирован для многопроцессорных сред с целью ускорения генерации эмбеддингов и работы моделей.

- **Сценарии применения:**

    Системы управления корпоративными документами.

    Автоматический ответ на вопросы из структурированных документов.

    Улучшенные системы поиска и извлечения данных из больших репозиториев .docx файлов.

---

### Установка и использование

1. Установите зависимости с помощью

    ```batch
    pip install -r requirements.txt
    ```

2. Подготовьте .docx файлы или возьмите готовую базу данных.

    ```py
    nlp = spacy.load('ru_core_news_sm')
    
    vs = create_empty_faiss_index(embed)
    
    docx2faiss([
        'doc1.docx',
        'doc2.docx',
        'doc3.docx',
    ], vs, nlp)
    ```

3. Генерируйте эмбеддинги и создавайте FAISS-индексы с помощью `embedding`.

    ```py
    embed = EmbeddingModel(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", half=True)
    ```
    
    Пример: скачать базу данных Википедии [здесь](https://drive.google.com/drive/folders/1-Tf7lbhTVmndmB5xKGTTE5r3eWNF8mih?usp=drive_link)

4. Загрузите языковую модель и задавайте вопросы через модуль `llm` для выполнения QA-задач.

    ```py
    llm = LLM("t-tech/T-lite-it-1.0")
    
    llm.answer(doc.page_content, "Твой интересный вопрос")
    ```
    
    Если возникают трудности с загрузкой моделей, то загрузи их [здесь](https://drive.google.com/drive/folders/1QRkNohUmA0CUdtLTA7yFdI8xErO5G4Jf?usp=drive_link)

5. (Опционально) Пользуйтесь Streamlit интерфейсом.

    ```batch
    streamlit run app.py
    ```
    
    Или запусти `run_app.bat`
    
    ![streamlit](https://github.com/user-attachments/assets/9ce18ed0-16ba-4449-8020-73fb44befbf3)

6. Если остаются вопросы, посмотри [блокнот с примером](example.ipynb)

---

### TODO

- Обучение TaBert от Facebook Research в качестве энкодера табличных данных - лучший энкодер для такого формата, тем более база данных собрана при помощи его парсера и совместима с требованиями модели. К сожалению, в виду ограниченности по времени и ресурсам, нам не удалось запустить процесс обучения (дообучение не имеет смысла, так оригинал полностью на английских токенах). Для обучения необходимо иметь минимум две мощные GPU, которых у нас на долгосрочной перспективе нет.
