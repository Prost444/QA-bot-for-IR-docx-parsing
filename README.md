# QA-bot-for-IR-docx-parsing

This repository contains the solution for the NLP case from Rosneft &amp; MIREA AI hackathon. It provides a method for parsing docx files or wiki_dump pages, extracting tables and lists for further processing and embedding them into a QA model.

Этот репозиторий предоставляет решение для обработки .docx документов, извлечения структурированных данных (например, таблиц и списков) и интеграции их в систему для ответа на вопросы с использованием современных NLP-моделей. Проект разработан для масштабируемой обработки документов и интеграции с трансформерными языковыми моделями.

---

### Постановка задачи:

Требовалось редложить метод парсинга таблиц и списков документов (docx) для векторной базы данных для IR-системы в составе QA чат-бота.
Для построения IR-системы с векторной базой данных было выбранно 2 подхода:

- Безлайн: собрать и обучить на табличных данных модель seq-to-seq. Модель должна была обучиться принимать текстовые запросы от пользователей и с помощью encoder-decoder архитектуры выдавать ответы с информацией из обучающей выборки.

- Прод: При помощи выбранного векторизатора создать FIASS базу данный с эмбеддингами табличных документов, по которой будет выполняться быстрый поиск контекста для запроса пользователя. Далее запрос и полученный из базы данных контекст подавался LLM для вычлинения нужной информации, суммаризации и представления ответа.

---

### Данные для обучения:

Оргонизаторами хакатона было предложено извлекать информацию о таблицах и списках для векторизованной базы данных из .docx файлов. Однако для создания информативной базы данных, а тем более обучения (дообучения) encoder-decoder модели требовалась выборка из большого количества docx файлов с миллионами информативных таблиц и списков. Найти подобную выборку нам не удалось, поэтому было принято решение создать ее самостоятельно на основе всех таблиц и списков, представленных в русской википедии.

# Шаги создания:

1. Установка последнего дампа русской википедии

   	В целях ускорения преобразования вики-файлов в docx было принято решение отказаться от парсинга интернет страниц впользу скачивания бэкапа и обработки всех страниц исключительно локально

2. Экстрация в docx файлы всех таблиц и списков с каждой страницы вики-дампа.

Код приведен в файле wiki_parse_to_docx.ipynb.

---

# Датасет для обучения

Для информативной реперезентации таблиц, понятной для nlp-систем все таблицы и списки было решено хранить в json формате (при этом все списки пребразуются к одномерным таблицам). Для сборки репрезентативного датасатена использованы 2 парсера (каталог preprocess):

1. docx2json.py - Собирает jsonl файл из предоставленного каталага с docx файлами.

2. extract_wiki_tables.sh - парсер от создателй Tabert, оптимизированный под новые версии питона и использование русского языка и захват списков. Парсит в jsonl файл напрямую из дампа википедии.

Оба парсера так же захватывают контекст, находящийся перед таблицей.
Такое извлечение информации показало отличные результаты на cpu с пиковой скоростью обработки до 300000 страниц в час. С их помощью были собраны все таблицы и списки русской фикипедии (файл https://drive.google.com/file/d/1vn8SfdPBgS6pA4fZLpyqhkemHtewENL7/view?usp=drive_link) и файл с исключительно информативными таблицами (мнимум 4 строки и 3 столбца) (https://drive.google.com/file/d/1w5K3LYRoLUIJ8TtdrkZbPjREf5rqj-NY/view?usp=drive_link)

---

### Обучение представлений:

Для бейзлайнового решения было принято дообучать DeepPavlon/rubert-base-cased для задачи seq-to-seq генерации по запросу. Код с построением нужной архитектуры и обучением приведен в файле llm-with-ir.ipynb. Для дообучния использовались первые 5000 строк из wiki_dump.jsonl. Модель получала текст из таблицы и генерировала контекст. Лосс - кросс энтропия. Графики представленны ниже
![telegram-cloud-photo-size-2-5388572639825621342-y](https://github.com/user-attachments/assets/81d420ce-8b01-4453-9d9a-e39a6bb03a15)
![telegram-cloud-photo-size-2-5388572639825621341-y](https://github.com/user-attachments/assets/cbf9cbbe-0d98-4815-8c71-ea59d316baef)

Из-за нехватки времени было невозможно обучить модель на большом кластере таблиц, из-за чего она начала переобучаться уже после 8ой эпохи. В дальнейшем мы использовали чекпоинт этой модели с минимальным eval-лоссом (до начала переобучения).

---

### Создание FAISS-INDEX:

Ключевым этапом создания векторизованной базы данных выступил выбор модели векторизации. Было протестировано множество вариантов готовых моделей c hagging face, использование encoder части дообученной seq-to-seq модели. Векторизировались как датасеты с информативными таблицами, так и полные датасеты. В ходе векторизации заголовок страницы (имя файла), контекст до таблицы, заголовки столбцов и форматированный текст таблицы (так же пробовали каждую строку отдельно) сливались в один текст и подвались энкодер модели. 

Индекс строился с использованием IndexFlatIP нормализации и близостью по косиносному расстоянию. На всех моделях результат поиска был неудовлетворительным (метрики посчитать не успели, но результат плохо соотносился с запросом). Кроме модели sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2, которая строила самые информативные эмбэддинги. Проверить качество поиска можно загрузив наш чекпоинт индекса и запустив блокнот handsome_fiass.ipynb.

Код создания векторизованной базы данных (FAISS-INDEX) 
на основе таблиц википедии приведен в файле wiki_to_index (рекоммендуем запускать на класстере из 2ух GPU T4). Код создания той же базы данных напрямую из docx файлов приведен в файле _docx_to_lang.py_. Готовые файлы для восстановки индекса, основанного на информативных таблицах википедии доступны по ссылке https://drive.google.com/drive/folders/1-Tf7lbhTVmndmB5xKGTTE5r3eWNF8mih?usp=drive_link.

---

### Дополнительная суммаризация при помощи LLM:

Для суммаризации ответов мы используем T-lite-it-1.0 - русскоязычную языковую модель от Т-банка с 8 миллиардами параметров. Модель разработана специально для задач, связанных с обработкой информации в корпоративной среде, что подходит для суммаризации отчётов компаний.

Мы выбрали 4-битную квантизацию для повышения скорости работы и уменьшения расхода памяти на архитектуре Cuda с видеокартой NVIDIA GTX 4070 Ti.

Bert-score:


---
### Summery:

- Парсинг документов:

	Использует библиотеку docx для извлечения структурированных данных, таких как таблицы и списки.

	Обрабатывает и форматирует извлеченные данные в единый контекст для использования в NLP-задачах.

- Работа с эмбеддингами:

	Реализована поддержка многогпу-вычислений с использованием библиотеки sentence-transformers.

	Обеспечивает создание, сохранение и загрузку FAISS-векторных индексов для эффективного поиска по схожести.

- Ответы на вопросы:

	Интеграция с трансформерными языковыми моделями с использованием библиотек transformers и torch.

	Настраиваемая генерация ответов на основе предоставленного контекста, поддерживая лаконичные и релевантные ответы.

- Ключевые компоненты:

	_docx_to_lang.py_: Обрабатывает .docx файлы, извлекает их содержимое (таблицы, списки, контекст) и преобразует их в совместимый формат для использования с Langchain.
	
	_llm.py_: Класс для взаимодействия с трансформерными языковыми моделями, обеспечивающий генерацию ответов на основе контекста и вопросов.
	
	_multi_gpu_embeddings.py_: Добавляет поддержку многогпу-вычислений для генерации эмбеддингов предложений и управления FAISS-векторными индексами.

- Технологический стек:

	Используемые библиотеки: torch, transformers, sentence-transformers, docx, faiss, langchain.

	Проект оптимизирован для многогпу-сред с целью ускорения генерации эмбеддингов и работы моделей.

- Сценарии применения:

	Системы управления корпоративными документами.

	Автоматический ответ на вопросы из структурированных документов.

	Улучшенные системы поиска и извлечения данных из больших репозиториев .docx файлов.

---

### Установка и использование

1. Установите зависимости с помощью

```batch
pip install -r requirements.txt
```

2. Подготовьте .docx файлы и обработайте их с помощью модуля docx_to_lang.

```py
nlp = spacy.load('<model>')

vs = create_empty_faiss_index(embedding_model)

docx2faiss([
	'doc1.docx',
	'doc2.docx',
	'doc3.docx',
], vs, nlp)
```

3. Генерируйте эмбеддинги с поддержкой GPU и создавайте FAISS-индексы с помощью _multi_gpu_embeddings_.

Пример: скачать базу данных википедии [здесь](https://drive.google.com/drive/folders/1-Tf7lbhTVmndmB5xKGTTE5r3eWNF8mih?usp=sharing)

4. Загрузите языковую модель и задавайте вопросы через модуль llm для выполнения QA-задач.

```py
llm = LLM("t-tech/T-lite-it-1.0")

llm.answer(doc.page_content,"Твой интересный вопрос")
```

---

### TODO

про таберт и тестирование
