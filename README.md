# QA-bot-for-IR-docx-parsing

This repository contains the solution for the NLP case from Rosneft &amp; MIREA AI hackathon. It provides a method for parsing docx files or wiki_dump pages, extracting tables and lists for further processing and embedding them into a QA model.

Этот репозиторий предоставляет решение для обработки .docx документов, извлечения структурированных данных (например, таблиц и списков) и интеграции их в систему для ответа на вопросы с использованием современных NLP-моделей. Проект разработан для масштабируемой обработки документов и интеграции с трансформерными языковыми моделями.

---

### Постановка задачи:

Требовалось редложить метод парсинга таблиц и списков документов (docx) для векторной базы данных для IR-системы в составе QA чат-бота.
Для построения IR-системы с векторной базой данных было выбранно 2 подхода:

- Безлайн: собрать и обучить на табличных данных модель seq-to-seq. Модель должна была обучиться принимать текстовые запросы от пользователей и с помощью encoder-decoder архитектуры выдавать ответы с информацией из обучающей выборки.

- Прод: При помощи выбранного векторизатора создать FIASS базу данный с эмбеддингами табличных документов, по которой будет выполняться быстрый поиск контекста для запроса пользователя. Далее запрос и полученный из базы данных контекст подавался LLM для вычлинения нужной информации, суммаризации и представления ответа.

---

### Данные для обучения:

Оргонизаторами хакатона было предложено извлекать информацию о таблицах и списках для векторизованной базы данных из .docx файлов. Однако для создания информативной базы данных, а тем более обучения (дообучения) encoder-decoder модели требовалась выборка из большого количества docx файлов с миллионами информативных таблиц и списков. Найти подобную выборку нам не удалось, поэтому было принято решение создать ее самостоятельно на основе всех таблиц и списков, представленных в русской википедии.

# Шаги создания:

1. Установка последнего дампа русской википедии

   	В целях ускорения преобразования вики-файлов в docx было принято решение отказаться от парсинга интернет страниц впользу скачивания бэкапа и обработки всех страниц исключительно локально

2. Экстрация в docx файлы всех таблиц и списков с каждой страницы вики-дампа.

Код приведен в файле wiki_parse_to_docx.ipynb.

---

# Датасет для обучения

Для информативной реперезентации таблиц, понятной для nlp-систем все таблицы и списки было решено хранить в json формате (при этом все списки пребразуются к одномерным таблицам). Для сборки репрезентативного датасатена использованы 2 парсера (каталог preprocess):

1. docx_to_json.py - Собирает jsonl файл из предоставленного каталага с docx файлами.

2. extract_wiki_tables.sh - парсер от создателей Tabert, оптимизированный под новые версии питона и использование русского языка и захват списков. Парсит в jsonl файл напрямую из дампа википедии.

Оба парсера так же захватывают контекст, находящийся перед таблицей.
Такое извлечение информации показало отличные результаты на cpu с пиковой скоростью обработки до 300000 страниц в час. С их помощью были собраны все таблицы и [списки русской фикипедии](https://drive.google.com/file/d/1vn8SfdPBgS6pA4fZLpyqhkemHtewENL7/view?usp=drive_link) и файл с исключительно [информативными таблицами](https://drive.google.com/file/d/1w5K3LYRoLUIJ8TtdrkZbPjREf5rqj-NY/view?usp=drive_link) (мнимум 4 строки и 3 столбца) 

---

### Обучение представлений:

Для бейзлайнового решения было принято дообучать DeepPavlon/rubert-base-cased для задачи seq-to-seq генерации по запросу. Код с построением нужной архитектуры и обучением приведен в файле llm-with-ir.ipynb. Для дообучния использовались первые 5000 строк из wiki_dump.jsonl. Модель получала текст из таблицы и генерировала контекст. Лосс - кросс энтропия. Графики представленны ниже
![telegram-cloud-photo-size-2-5388572639825621342-y](https://github.com/user-attachments/assets/81d420ce-8b01-4453-9d9a-e39a6bb03a15)
![telegram-cloud-photo-size-2-5388572639825621341-y](https://github.com/user-attachments/assets/cbf9cbbe-0d98-4815-8c71-ea59d316baef)

Из-за нехватки времени было невозможно обучить модель на большом кластере таблиц, из-за чего она начала переобучаться уже после 8ой эпохи. В дальнейшем мы использовали чекпоинт этой модели с минимальным eval-лоссом (до начала переобучения).

Метрики по этой модели:

Bert score:

|precision|recall|f1|
|-|-|-|
|0.41671923166368063|0.5871573090553284|0.48525058496289136|

|Rouge-L-P|Rouge-L-R|Rouge-L-F|
|-|-|-|
|0.0031532775377011084|0.7919307892145258|0.7919307892145258|


---

### Создание FAISS-INDEX:

Ключевым этапом создания векторизованной базы данных выступил выбор модели векторизации. Было протестировано множество вариантов готовых моделей c hagging face, использование encoder части дообученной seq-to-seq модели. Векторизировались как датасеты с информативными таблицами, так и полные датасеты. В ходе векторизации заголовок страницы (имя файла), контекст до таблицы, заголовки столбцов и форматированный текст таблицы (так же пробовали каждую строку отдельно) сливались в один текст и подвались энкодер модели. 

Индекс строился с использованием IndexFlatIP нормализации и близостью по косиносному расстоянию. На всех моделях результат поиска был неудовлетворительным (почему - покажем в разделе про метрики). Кроме модели sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2, которая строила самые информативные эмбэддинги. Проверить качество поиска можно загрузив наш чекпоинт индекса и запустив блокнот handsome_fiass.ipynb (для отдельного запроса). Так же вы можете создать собственный индекс на основе ваших docx файлов с нашим векторизатором при помощи модуля nlp

Код создания векторизованной базы данных (FAISS-INDEX) 
на основе таблиц википедии приведен в файле wiki_to_index (рекоммендуем запускать на класстере из 2ух GPU T4). Код создания той же базы данных напрямую из docx файлов приведен в файле _nlp.py_. Готовые файлы для восстановки индекса, основанного на [информативных таблицах википедии](https://drive.google.com/drive/folders/1-Tf7lbhTVmndmB5xKGTTE5r3eWNF8mih?usp=drive_link) .

---

### Дополнительная суммаризация при помощи LLM:

Для суммаризации ответов мы используем [T-lite-it-1.0](https://huggingface.co/t-tech/T-lite-it-1.0) - самую новую русскоязычную языковую модель от Т-банка с 8 миллиардами параметров. Модель разработана специально для задач, связанных с обработкой информации в корпоративной среде, что подходит для суммаризации отчётов компаний. Ее интеграция для последующих вызовов приведена в модуле llm.

Мы выбрали 4-битную квантизацию для повышения скорости работы и уменьшения расхода памяти на архитектуре Cuda с видеокартой NVIDIA GTX 4070 Ti.

Bert-score:
|precision|recall|f1|
|-|-|-|
|0.6252651592580284|0.5997003534945046|0.6115292179875258|

Rouge-L-P|Rouge-L-R|Rouge-L-F
|-|-|-|
|0.002885014107601106|0.9669236991590445|0.9669236991590445|


---
### Метрики:
Выбор метрик в подобных задачах – это всегда баланс между практичностью, интерпретируемостью и согласованностью с реальными критериями качества ответа. Несмотря на то, что на рынке существует широкий спектр метрик, мы сосредоточились на BertScore и вариантах Rouge-L не случайно:

Сочетание лексического и семантического анализа:

Rouge-L измеряет степень лексического пересечения генерации с эталонным ответом. Оно отслеживает Longest Common Subsequence (LCS) между ответом модели и референсом, что позволяет оценить непрерывные фрагменты совпадений, а не просто отдельные токены. Это особенно важно, когда необходимо убедиться, что ответ не просто содержит отдельные релевантные слова, а структурно приближен к эталону (и не повторяет запрос точь-в-точь).

BertScore, в свою очередь, фокусируется не только на лексическом совпадении, но и на семантической близости между ответом модели и референсами. Данная метрика использует эмбеддинги, полученные из языковой модели, чтобы сопоставить токены на уровне смысловых векторов, а не исключительно их лексическую форму. Таким образом, BertScore может «прощать» перефразирование, синонимы и альтернативные грамматические конструкции, если смысл сохраняется.

В отношении RAG симтем топ по метрикам выстроился довольно прозрачно следующим образом:

|                            | bert-pr            | bert-re            | bert-f1            | Rouge-L-P           | Rouge-L-R          | Rouge-L-F          |
|----------------------------|--------------------|--------------------|--------------------|---------------------|--------------------|--------------------|
| paraphrase-multilingual    | 0.6252             | 0.5997             | 0.6115             | 0.00285         | 0.966923           | 0.9669236          |
| DeepPavlov_for_Panacea     | 0.5903             | 0.5655             | 0.5778             | 0.00288         | 0.9669200          | 0.9669200          |
| sentence-rubert             | 0.5899             | 0.5652             | 0.5775             | 0.00288          | 0.9669198          | 0.9669198          |
| sentence-rubert на строках | 0.5896             | 0.5649             | 0.5772             | 0.00289          | 0.9669196          | 0.9669196          |
| наш-дообученный-энкодер    | 0.5894             | 0.5647             | 0.5770             | 0.00302          | 0.9669194          | 0.9669194          |


Так же видим колоссальный прирост в осознанности и качестве нового решения:

| Metric       | Seq-to-Seq            | RAG System             | Change (%) |
|--------------|-----------------------|------------------------|------------|
| Bert-Precision | 0.4167               | 0.6253                 | +50.0%     |
| Bert-Recall    | 0.5872               | 0.5997                 | +2.1%      |
| Bert-F1        | 0.4853               | 0.6115                 | +26.0%     |
| Rouge-L-P       | 0.003153             | 0.002885               | -9.3%      |
| Rouge-L-R       | 0.7919               | 0.9669                  | +18.1%     |
| Rouge-L-F       | 0.7919               | 0.9669                 | +18.1%     |

Такие низкие значения Rouge-L-P обусловленны тем, что модель использует минимум токенов из запроса. То есть креативит и выдает длинный ответ с новыми словами.

---
### Summery:

- Парсинг документов:

	Использует библиотеку docx для извлечения структурированных данных, таких как таблицы и списки.

	Обрабатывает и форматирует извлеченные данные в единый контекст для использования в NLP-задачах.

	Чтобы загрузить Русскую NLP модель, выполни команду
```batch
python -m spacy download ru_core_news_sm
```

- Работа с эмбеддингами:

	Реализована поддержка многогпу-вычислений с использованием библиотеки sentence-transformers.

	Обеспечивает создание, сохранение и загрузку FAISS-векторных индексов для эффективного поиска по схожести.

- Ответы на вопросы:

	Интеграция с трансформерными языковыми моделями с использованием библиотек transformers и torch.

	Настраиваемая генерация ответов на основе предоставленного контекста, поддерживая лаконичные и релевантные ответы.

- Ключевые компоненты:

	_nlp.py_: Обрабатывает .docx файлы, извлекает их содержимое (таблицы, списки, контекст) и преобразует их в совместимый формат для использования с Langchain.
	
	_llm.py_: Класс для взаимодействия с трансформерными языковыми моделями, обеспечивающий генерацию ответов на основе контекста и вопросов.
	
	_multi_gpu_embeddings.py_: Добавляет поддержку многогпу-вычислений для генерации эмбеддингов предложений и управления FAISS-векторными индексами.

- Технологический стек:

	Используемые библиотеки: torch, transformers, sentence-transformers, docx, faiss, langchain.

	Проект оптимизирован для многогпу-сред с целью ускорения генерации эмбеддингов и работы моделей.

- Сценарии применения:

	Системы управления корпоративными документами.

	Автоматический ответ на вопросы из структурированных документов.

	Улучшенные системы поиска и извлечения данных из больших репозиториев .docx файлов.

---

### Установка и использование

1. Установите зависимости с помощью

```batch
pip install -r requirements.txt
```

2. Подготовьте .docx файлы или возьмите готовую базу данных.

```py
nlp = spacy.load('ru_core_news_sm')

vs = create_empty_faiss_index(embed)

docx2faiss([
	'doc1.docx',
	'doc2.docx',
	'doc3.docx',
], vs, nlp)
```

3. Генерируйте эмбеддинги и создавайте FAISS-индексы с помощью _embedding_.

```py
embed = EmbeddingModel(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", half=True)
```

Пример: скачать базу данных википедии [здесь](https://drive.google.com/drive/folders/1-Tf7lbhTVmndmB5xKGTTE5r3eWNF8mih?usp=drive_link)

4. Загрузите языковую модель и задавайте вопросы через модуль llm для выполнения QA-задач.

```py
llm = LLM("t-tech/T-lite-it-1.0")

llm.answer(doc.page_content,"Твой интересный вопрос")
```

Если возникают трудности с загрузкой моделей, то загрузи их [здесь](https://drive.google.com/drive/folders/1QRkNohUmA0CUdtLTA7yFdI8xErO5G4Jf?usp=drive_link)

5. (Опционально) Пользуйтесь streamlit интерфейсом.

```batch
streamlit run app.py
```

Или запусти run_app.bat

![streamlit](https://github.com/user-attachments/assets/9ce18ed0-16ba-4449-8020-73fb44befbf3)

6. Если остаются вопросы, посмотри [блокнот с примером](example.ipynb)

---

### TODO

- Обучение TaBert от FacebookResearch в качестве энкодера табличных данных - лучший энкодер для такого формата, тем более база данных собрана при помощи его парсера и совместима с требованиями модели. К сожалению, в виду ограниченности по времени и ресурсам, нам не удалось запустить процесс обучения (дообучать не имеет смысла, так оригинал полностью на английских токенах). Для обучения необходимо иметь минимум две мощные гпу, которых у на с долгосрочной перспективе нет.
