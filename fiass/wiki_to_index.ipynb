{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10170794,"sourceType":"datasetVersion","datasetId":6281445},{"sourceId":10174404,"sourceType":"datasetVersion","datasetId":6284201},{"sourceId":10183147,"sourceType":"datasetVersion","datasetId":6290599}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install langchain langchain-community faiss-gpu sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-13T01:54:24.952848Z","iopub.execute_input":"2024-12-13T01:54:24.953097Z","iopub.status.idle":"2024-12-13T01:54:42.586529Z","shell.execute_reply.started":"2024-12-13T01:54:24.953071Z","shell.execute_reply":"2024-12-13T01:54:42.585506Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\nCollecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.5)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.4.0,>=0.3.24 (from langchain)\n  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.3,>=0.1.17 (from langchain)\n  Downloading langsmith-0.2.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.10.1)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.3.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.7)\nCollecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.26.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\nCollecting packaging>=20.9 (from huggingface-hub>=0.20.0->sentence-transformers)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.4)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.6.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (2.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.0)\nDownloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\nDownloading langsmith-0.2.3-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\nDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu, packaging, httpx-sse, requests-toolbelt, pydantic-settings, langsmith, langchain-core, sentence-transformers, langchain-text-splitters, langchain, langchain-community\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.10.1 requires cubinlinker, which is not installed.\ncudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.10.1 requires libcudf==24.10.*, which is not installed.\ncudf 24.10.1 requires ptxcompiler, which is not installed.\ncuml 24.10.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 24.10.0 requires cuvs==24.10.*, which is not installed.\ncuml 24.10.0 requires nvidia-cublas, which is not installed.\ncuml 24.10.0 requires nvidia-cufft, which is not installed.\ncuml 24.10.0 requires nvidia-curand, which is not installed.\ncuml 24.10.0 requires nvidia-cusolver, which is not installed.\ncuml 24.10.0 requires nvidia-cusparse, which is not installed.\ndask-cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\npylibcudf 24.10.1 requires libcudf==24.10.*, which is not installed.\ncudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\ncudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ndask-cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ndistributed 2024.9.0 requires dask==2024.9.0, but you have dask 2024.11.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\njupyterlab 4.3.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.3 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npylibcudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\nrapids-dask-dependency 24.10.0a0 requires dask==2024.9.0, but you have dask 2024.11.2 which is incompatible.\nrapids-dask-dependency 24.10.0a0 requires dask-expr==1.1.14, but you have dask-expr 1.1.19 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.12.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed faiss-gpu-1.7.2 httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 langchain-core-0.3.24 langchain-text-splitters-0.3.2 langsmith-0.2.3 packaging-24.2 pydantic-settings-2.6.1 requests-toolbelt-1.0.0 sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T01:54:42.588889Z","iopub.execute_input":"2024-12-13T01:54:42.589166Z","iopub.status.idle":"2024-12-13T01:54:42.645521Z","shell.execute_reply.started":"2024-12-13T01:54:42.589139Z","shell.execute_reply":"2024-12-13T01:54:42.644703Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ruwiki-tables-and-lists/wiki_tables_and_lists.jsonl\n/kaggle/input/ruwiki-valid-tables/wiki_dump.jsonl\n/kaggle/input/bert2bert-4b-300e/results/runs/Dec11_01-51-37_075c355daadc/events.out.tfevents.1733881900.075c355daadc.23.0\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/config.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/trainer_state.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/training_args.bin\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/tokenizer.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/tokenizer_config.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/scheduler.pt\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/model.safetensors\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/special_tokens_map.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/optimizer.pt\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/vocab.txt\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/rng_state.pth\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-9912/generation_config.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/config.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/trainer_state.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/training_args.bin\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/tokenizer.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/tokenizer_config.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/scheduler.pt\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/model.safetensors\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/special_tokens_map.json\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/optimizer.pt\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/vocab.txt\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/rng_state.pth\n/kaggle/input/bert2bert-4b-300e/results/checkpoint-50976/generation_config.json\n/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/run-qeoxzw3k.wandb\n/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/logs/debug.log\n/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/logs/debug-internal.log\n/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/files/output.log\n/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/files/requirements.txt\n/kaggle/input/bert2bert-4b-300e/wandb/run-20241211_015140-qeoxzw3k/files/wandb-metadata.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport json\nfrom typing import List\nfrom tqdm import tqdm\nimport torch\nfrom torch import nn\n\nfrom langchain.docstore.document import Document as LangchainDocument\nfrom langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\nfrom transformers import AutoTokenizer, AutoModel\nfrom pydantic import PrivateAttr\nimport re\n\n# Параметры\nEMBEDDING_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\nINPUT_FILE = \"/kaggle/input/ruwiki-valid-tables/wiki_dump.jsonl\"\nOUTPUT_DIR = \"/kaggle/working\"\nCHECKPOINT_NAME = \"faiss_tables_index_checkpoint\"\nBATCH_SIZE = 1000\nCHECKPOINT_INTERVAL = 10\nRESUME_FROM_CHECKPOINT = True\n\nclass MultiGPUHuggingFaceEmbeddings(HuggingFaceEmbeddings):\n    _tokenizer: AutoTokenizer = PrivateAttr()\n    _model: nn.Module = PrivateAttr()\n    _device: str = PrivateAttr()\n    _half: bool = PrivateAttr()\n    _normalize_embeddings: bool = PrivateAttr(default=False)\n\n    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME, device: str = None, half: bool = True, **kwargs):\n        super().__init__(model_name=model_name, **kwargs)\n        \n        encode_kwargs = kwargs.get(\"encode_kwargs\", {})\n        self._normalize_embeddings = encode_kwargs.get(\"normalize_embeddings\", True)  # включаем нормализацию\n\n        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self._model = AutoModel.from_pretrained(model_name)\n        self._model.eval()\n\n        # Проверка на количество GPU\n        if torch.cuda.device_count() > 1:\n            print(f\"Using {torch.cuda.device_count()} GPUs for inference...\")\n            self._model = nn.DataParallel(self._model)\n            self._device = 'cuda'\n        else:\n            self._device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        self._model.to(self._device)\n\n        if half and 'cuda' in self._device:\n            self._model.half()\n\n        self._half = half\n\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        # Используем mean pooling для получения sentence embeddings\n        batch_size = 256\n        embeddings = []\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n            inputs = self._tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n            inputs = {k: v.to(self._device) for k,v in inputs.items()}\n\n            with torch.no_grad():\n                outputs = self._model(**inputs)\n                last_hidden_state = outputs.last_hidden_state\n                attention_mask = inputs['attention_mask']\n\n                # Mean Pooling\n                input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n                sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n                sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n                cls_emb = (sum_embeddings / sum_mask).float().cpu().numpy()\n\n                if self._normalize_embeddings:\n                    norm = (cls_emb**2).sum(axis=1, keepdims=True)**0.5\n                    cls_emb = cls_emb / norm\n                embeddings.extend(cls_emb.tolist())\n        return embeddings\n\n    def embed_query(self, text: str) -> List[float]:\n        return self.embed_documents([text])[0]\n\ndef create_empty_faiss_index(embedding_model, distance_strategy=DistanceStrategy.COSINE):\n    import faiss\n    from langchain.docstore.in_memory import InMemoryDocstore\n\n    # Получаем размерность через фиктивный текст\n    dummy_emb = embedding_model.embed_documents([\"hello\"])\n    dim = len(dummy_emb[0])\n\n    if distance_strategy == DistanceStrategy.COSINE:\n        index = faiss.IndexFlatIP(dim)\n    else:\n        index = faiss.IndexFlatL2(dim)\n\n    docstore = InMemoryDocstore({})\n    vectorstore = FAISS(\n        index=index,\n        docstore=docstore,\n        index_to_docstore_id={},\n        embedding_function=embedding_model,\n        distance_strategy=distance_strategy\n    )\n    return vectorstore\n\ndef create_or_load_faiss_index(embedding_model):\n    checkpoint_path = os.path.join(OUTPUT_DIR, CHECKPOINT_NAME)\n    if RESUME_FROM_CHECKPOINT and os.path.exists(checkpoint_path):\n        print(f\"Loading existing FAISS index from {checkpoint_path}\")\n        vectorstore = FAISS.load_local(checkpoint_path, embedding_model, allow_dangerous_deserialization=True)\n    else:\n        vectorstore = create_empty_faiss_index(embedding_model, distance_strategy=DistanceStrategy.COSINE)\n    return vectorstore\n\ndef save_faiss_index(vectorstore):\n    checkpoint_path = os.path.join(OUTPUT_DIR, CHECKPOINT_NAME)\n    vectorstore.save_local(checkpoint_path)\n    print(f\"Checkpoint saved at {checkpoint_path}\")\n\ndef main():\n    embedding_model = MultiGPUHuggingFaceEmbeddings(\n        model_name=EMBEDDING_MODEL_NAME,\n        half=True,\n        model_kwargs={\"device\": \"cuda\"},\n        encode_kwargs={\"normalize_embeddings\": True},\n        multi_process=False\n    )\n\n    vectorstore = create_or_load_faiss_index(embedding_model)\n\n    processed_tables_count = 0\n    offset_file = os.path.join(OUTPUT_DIR, \"offset.txt\")\n    if RESUME_FROM_CHECKPOINT and os.path.exists(offset_file):\n        with open(offset_file, \"r\") as f:\n            processed_tables_count = int(f.read().strip())\n        print(f\"Resuming from table #{processed_tables_count}\")\n\n    docs_batch = []\n    batch_counter = 0\n\n    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n        for _ in range(processed_tables_count):\n            f.readline()\n\n        for line in tqdm(f, desc=\"Processing tables\"):\n            line = line.strip()\n            if not line:\n                continue\n            entry = json.loads(line)\n            uuid = entry.get(\"uuid\", \"\")\n            context_before = entry.get(\"context_before\", \"\") or \"\"\n            caption = entry.get(\"caption\", \"\") or \"\"\n            header = entry.get(\"header\", [])\n            data = entry.get(\"data\", [])\n            uuid_text = re.sub(r'[^А-Яа-яЁё_]+', '', uuid)\n            uuid_text = uuid_text.replace('_', ' ')\n            context_text = '\\n'.join('\\n'.join(inner_text) for inner_text in context_before)\n            header_names = [col[\"name\"] for col in header]\n            header_line = \" | \".join(header_names)\n\n            table_lines = []\n            for row in data:\n                row_values = [cell[1] for cell in row] if row and isinstance(row[0], list) else row\n                table_lines.append(\" | \".join(row_values))\n            table_text = \"\\n\".join(table_lines)\n\n            full_text = f\"{uuid_text}\\n{context_text}\\n{caption}\\n{header_line}\\n{table_text}\".strip()\n            \n            doc = LangchainDocument(\n                page_content=full_text,\n                metadata={\n                    \"uuid\": uuid,\n                    \"header\": header_names\n                }\n            )\n            docs_batch.append(doc)\n\n            processed_tables_count += 1\n\n            if len(docs_batch) >= BATCH_SIZE:\n                vectorstore.add_documents(docs_batch)\n                docs_batch = []\n                batch_counter += 1\n\n                if batch_counter % CHECKPOINT_INTERVAL == 0:\n                    save_faiss_index(vectorstore)\n                    with open(offset_file, \"w\") as f_off:\n                        f_off.write(str(processed_tables_count))\n                    print(f\"Processed {processed_tables_count} tables so far.\")\n            # if processed_tables_count>=30000:\n            #     break\n\n    if docs_batch:\n        vectorstore.add_documents(docs_batch)\n\n    save_faiss_index(vectorstore)\n    with open(offset_file, \"w\") as f_off:\n        f_off.write(str(processed_tables_count))\n    print(f\"Final processed tables: {processed_tables_count}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T02:01:54.683356Z","iopub.execute_input":"2024-12-13T02:01:54.683729Z","iopub.status.idle":"2024-12-13T02:19:22.704522Z","shell.execute_reply.started":"2024-12-13T02:01:54.683698Z","shell.execute_reply":"2024-12-13T02:19:22.703076Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs for inference...\nLoading existing FAISS index from /kaggle/working/faiss_tables_index_checkpoint\nResuming from table #30000\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 10905it [00:29, 414.06it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 40000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 20927it [00:58, 415.64it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 50000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 30804it [01:27, 403.46it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 60000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 40678it [01:57, 352.71it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 70000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 50914it [02:28, 341.48it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 80000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 60581it [02:59, 341.33it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 90000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 70665it [03:30, 332.62it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 100000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 80749it [04:01, 346.95it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 110000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 90907it [04:34, 324.05it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 120000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 100858it [05:05, 313.33it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 130000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 110354it [05:38, 198.95it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 140000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 120339it [06:09, 227.51it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 150000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 130598it [06:42, 292.96it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 160000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 140794it [07:14, 283.28it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 170000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 150632it [07:48, 254.95it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 180000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 160689it [08:20, 265.65it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 190000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 170850it [08:52, 272.36it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 200000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 180611it [09:24, 261.26it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 210000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 190463it [09:58, 230.15it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 220000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 200147it [10:32, 182.36it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 230000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 210638it [11:06, 251.12it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 240000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 220345it [11:41, 149.98it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 250000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 230541it [12:15, 213.97it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 260000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 240614it [12:48, 241.59it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 270000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 250860it [13:25, 224.82it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 280000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 260706it [14:02, 235.40it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 290000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 270638it [14:40, 207.28it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 300000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 280682it [15:16, 212.32it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 310000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 290514it [15:53, 186.60it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 320000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 300308it [16:30, 118.34it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 330000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 310835it [17:06, 210.30it/s]","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nProcessed 340000 tables so far.\n","output_type":"stream"},{"name":"stderr","text":"Processing tables: 312658it [17:12, 302.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved at /kaggle/working/faiss_tables_index_checkpoint\nFinal processed tables: 342658\n","output_type":"stream"}],"execution_count":4}]}